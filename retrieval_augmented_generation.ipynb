{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91JuB53SQO-b",
        "outputId": "1c8b9d64-e2dd-4e05-9b75-1c442cf36c5c"
      },
      "outputs": [],
      "source": [
        "! pip install -U langchain_community langchain datasets faiss-cpu gradio langchain_experimental rank_bm25 sentence_transformers evaluate nltk rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7acGTbXKQY6-",
        "outputId": "188ee202-197f-4a3c-b419-539e8af52143"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"Using device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whqa6isgQbMW",
        "outputId": "3718cf76-993f-4a21-f463-62e601541199"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Device set to use cuda:0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (823 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence Abraham Lincoln is the sixteenth \n",
            " expected  President of the United States \n",
            " predicted Abraham Lincoln (February 12, 1809 Ã¢x80x93 April 15, 1865) was the sixteenth President of the United States, serving from March 4, 1861 until his assassination.\n",
            "sentence The Uruguayan constitution allows citizens to challenge laws \n",
            " expected  approved by Parliament by use of a Referendum, or to propose changes to the Constitution by the use of a Plebiscite. \n",
            " predicted The Uruguayan constitution allows citizens to challenge laws approved by Parliament by use of a Referendum, or to propose changes to the Constitution by the use of a Plebiscite. During the last 15 years the method has been used several times; to confirm an amnesty to members of the military who violated human rights during the military regime (1973-1985), to stop privatization of public utilities companies (See Economy: Public Sector), to defend pensioners' incomes, and to protect water resources.\"\n",
            "sentence John Adams, Jr. (October 30,1735 July 4, 1826) was the second \n",
            " expected President of the United States (1797 1801). He also served as America's first Vice President (1789 1797). \n",
            " predicted John Adams, Jr. (October 30, 1735 July 4, 1826) was the second President of the United States (1797 1801). He also served as America's first Vice President (1789 1797).\n",
            "\n",
            "Average Exact Match: 0.00\n",
            "\n",
            "Average Rouge: 0.50\n",
            "\n",
            "Average Meteor: 0.75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from datasets import load_dataset\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.documents import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.load import dumps, loads\n",
        "from operator import itemgetter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from sentence_transformers import CrossEncoder\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from evaluate import load\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Create LLM and Embeddings\n",
        "def initialize_llm_and_embeddings():\n",
        "    llm = HuggingFacePipeline(\n",
        "        pipeline=pipeline(\n",
        "            \"text2text-generation\",\n",
        "            model=\"google/flan-t5-base\",\n",
        "            max_new_tokens=256,\n",
        "        )\n",
        "    )\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return llm, embeddings\n",
        "\n",
        "\n",
        "\n",
        "def load_documents():\n",
        "    ds = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
        "    docs = [Document(page_content=entry[\"passage\"]) for entry in ds[\"passages\"]]\n",
        "    return docs\n",
        "\n",
        "\n",
        "def split_documents(docs, embeddings):\n",
        "    splitter = SemanticChunker(\n",
        "        embeddings=embeddings,\n",
        "        breakpoint_threshold_type=\"percentile\",\n",
        "        breakpoint_threshold_amount=95\n",
        "    )\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "def create_retrievers(splits, embeddings):\n",
        "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":50})\n",
        "    bm25_retriever = BM25Retriever.from_documents(splits)\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "        retrievers=[retriever, bm25_retriever],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    return ensemble_retriever\n",
        "\n",
        "\n",
        "def create_query_generator(llm):\n",
        "    template = \"\"\"You are an AI language model assistant. Your task is to generate ten\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    generate_queries = (\n",
        "        prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "        | (lambda x: x.split(\"\\n\"))\n",
        "    )\n",
        "    return generate_queries\n",
        "\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "def create_reranking_retriever(base_retriever, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_k = 5):\n",
        "    model = HuggingFaceCrossEncoder(model_name=model_name)\n",
        "    reranker = CrossEncoderReranker(model=model, top_n = top_k)\n",
        "    compression_retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=base_retriever)\n",
        "    return compression_retriever\n",
        "\n",
        "\n",
        "def print_context(inputs):\n",
        "        context = inputs[\"context\"]\n",
        "        print(\"\\n--- Retrieved Context ---\\n\")\n",
        "        print(context)\n",
        "        print(\"\\n-------------------------\\n\")\n",
        "        return inputs\n",
        "\n",
        "\n",
        "def build_rag_chain(llm, retrieval_chain):\n",
        "    prompt_template = \"\"\"\n",
        "You are an AI assistant. Given an incomplete sentence and some background context, complete the sentence in two to three grammatically correct and fluent English sentences.\n",
        "\n",
        "Use the context provided below to make the continuation logically consistent, factually informative, and relevant.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Incomplete Sentence:\n",
        "{sentence}\n",
        "\n",
        "Completion:\n",
        "\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "    rag_chain = (\n",
        "        {\"context\": retrieval_chain, \"sentence\": itemgetter(\"sentence\") | RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return rag_chain\n",
        "\n",
        "def compute_metrics(predicted, expected):\n",
        "  exact_match_metric = load(\"exact_match\")\n",
        "  rouge_metric = load(\"rouge\")\n",
        "  meteor_metric = load(\"meteor\")\n",
        "\n",
        "  predicted = list(map(lambda x: str(x) if x is not None else \"\", predicted))\n",
        "  expected = list(map(lambda x: str(x) if x is not None else \"\", expected))\n",
        "\n",
        "  exact_match_metric.add_batch(predictions=predicted, references=expected)\n",
        "  rouge_metric.add_batch(predictions=predicted, references=expected)\n",
        "  meteor_metric.add_batch(predictions=predicted, references=expected)\n",
        "\n",
        "  em = exact_match_metric.compute()[\"exact_match\"]\n",
        "  rouge = rouge_metric.compute()[\"rougeL\"]\n",
        "  meteor = meteor_metric.compute()[\"meteor\"]\n",
        "\n",
        "\n",
        "  return em,rouge,meteor\n",
        "\n",
        "def evaluate(rag_chain, test_data):\n",
        "  all_predicted = []\n",
        "  all_expected = []\n",
        "  results = []\n",
        "  for item in test_data:\n",
        "    sentence = item[\"sentence\"]\n",
        "    expected = item[\"completion\"]\n",
        "    predicted = rag_chain.invoke({\"sentence\":sentence})\n",
        "\n",
        "    print(f\"sentence {sentence} \\n expected {expected} \\n predicted {predicted}\")\n",
        "\n",
        "    all_predicted.append(str(predicted) if predicted is not None else \"\")\n",
        "    all_expected.append(str(expected) if expected is not None else \"\")\n",
        "    results.append({\n",
        "        \"sentence\":sentence,\n",
        "        \"expected_completion\":expected,\n",
        "        \"predicted_completion\":predicted\n",
        "    })\n",
        "\n",
        "  em, rouge, meteor = compute_metrics(all_predicted, all_expected)\n",
        "  print(f\"\\nAverage Exact Match: {em:.2f}\")\n",
        "  print(f\"\\nAverage Rouge: {rouge:.2f}\")\n",
        "  print(f\"\\nAverage Meteor: {meteor:.2f}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    test_data = [\n",
        "    {\"sentence\": \"Abraham Lincoln is the sixteenth\", \"completion\": \" President of the United States\"},\n",
        "    {\"sentence\": \"The Uruguayan constitution allows citizens to challenge laws\", \"completion\": \" approved by Parliament by use of a Referendum, or to propose changes to the Constitution by the use of a Plebiscite.\"},\n",
        "    {\"sentence\": \"John Adams, Jr. (October 30,1735 July 4, 1826) was the second\", \"completion\": \"President of the United States (1797 1801). He also served as America's first Vice President (1789 1797).\"}\n",
        "]\n",
        "\n",
        "    llm, embeddings = initialize_llm_and_embeddings()\n",
        "    docs = load_documents()\n",
        "    splits = split_documents(docs, embeddings)\n",
        "    ensemble_retriever = create_retrievers(splits, embeddings)\n",
        "    reranking_retriever = create_reranking_retriever(ensemble_retriever)\n",
        "    generate_queries = create_query_generator(llm)\n",
        "\n",
        "\n",
        "    retrieval_chain = (\n",
        "      RunnableLambda(lambda x: reranking_retriever.get_relevant_documents(x[\"sentence\"]))\n",
        "      | RunnableLambda(lambda docs: {\"context\": get_unique_union([docs])})\n",
        "    )\n",
        "    rag_chain = build_rag_chain(llm, retrieval_chain)\n",
        "\n",
        "\n",
        "    test_results = evaluate(rag_chain= rag_chain, test_data=test_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1FTnUB-Qk7g"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def answer_question(question):\n",
        "  answer = rag_chain.invoke({\"question\": question})\n",
        "  return answer\n",
        "\n",
        "llm, embeddings = initialize_llm_and_embeddings()\n",
        "docs = load_documents()\n",
        "splits = split_documents(docs, embeddings)\n",
        "ensemble_retriever = create_retrievers(splits, embeddings)\n",
        "reranking_retriever = create_reranking_retriever(ensemble_retriever)\n",
        "generate_queries = create_query_generator(llm)\n",
        "\n",
        "retrieval_chain = generate_queries | reranking_retriever.map() | get_unique_union\n",
        "rag_chain = build_rag_chain(llm, retrieval_chain)\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Demo\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

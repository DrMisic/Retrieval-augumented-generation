{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91JuB53SQO-b",
        "outputId": "1b8bbc85-3d9e-4ba4-99ea-56b28d96fa5a"
      },
      "outputs": [],
      "source": [
        "! pip install -U langchain_community langchain datasets faiss-cpu gradio langchain_experimental rank_bm25 sentence_transformers evaluate nltk rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpb8H7BKjApN",
        "outputId": "71d9b1ec-1fb8-4a76-b9a2-b9905ff906fa"
      },
      "outputs": [],
      "source": [
        "pip list | grep -E 'langchain_community|tiktoken|langchain-openai|langchainhub|chromadb|langchain$|datasets|faiss-cpu|gradio|langchain_experimental|rank_bm25|sentence_transformers'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7acGTbXKQY6-",
        "outputId": "2c164ece-57c7-4d89-a3ab-bdc7b174f918"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"Using device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whqa6isgQbMW",
        "outputId": "6134f178-261c-4de4-80d9-74625c893a7d"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from datasets import load_dataset\n",
        "from langchain_core.documents import Document\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.load import dumps, loads\n",
        "from operator import itemgetter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from sentence_transformers import CrossEncoder\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "from evaluate import load\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "def initialize_llm_and_embeddings():\n",
        "    llm = HuggingFacePipeline(\n",
        "        pipeline=pipeline(\n",
        "            \"text2text-generation\",\n",
        "            model=\"google/flan-t5-base\",\n",
        "            max_new_tokens=256,\n",
        "            model_kwargs={\"temperature\": 0.1},\n",
        "        )\n",
        "    )\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return llm, embeddings\n",
        "\n",
        "\n",
        "\n",
        "def load_documents():\n",
        "    ds = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
        "    docs = [Document(page_content=entry[\"passage\"]) for entry in ds[\"passages\"]]\n",
        "    print(docs)\n",
        "    return docs\n",
        "\n",
        "\n",
        "def split_documents(docs, embeddings):\n",
        "    splitter = SemanticChunker(\n",
        "        embeddings=embeddings,\n",
        "        breakpoint_threshold_type=\"percentile\",\n",
        "        breakpoint_threshold_amount=95\n",
        "    )\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "def create_retrievers(splits, embeddings):\n",
        "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":50})\n",
        "    bm25_retriever = BM25Retriever.from_documents(splits)\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "        retrievers=[retriever, bm25_retriever],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "    return ensemble_retriever\n",
        "\n",
        "\n",
        "def create_query_generator(llm):\n",
        "    template = \"\"\"You are an AI language model assistant. Your task is to generate ten\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    generate_queries = (\n",
        "        prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "        | (lambda x: x.split(\"\\n\"))\n",
        "    )\n",
        "    return generate_queries\n",
        "\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "def create_reranking_retriever(base_retriever, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_k = 5):\n",
        "    model = HuggingFaceCrossEncoder(model_name=model_name)\n",
        "    reranker = CrossEncoderReranker(model=model, top_n = top_k)\n",
        "    compression_retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=base_retriever)\n",
        "    return compression_retriever\n",
        "\n",
        "\n",
        "def print_context(inputs):\n",
        "        context = inputs[\"context\"]\n",
        "        print(\"\\n--- Retrieved Context ---\\n\")\n",
        "        print(context)\n",
        "        print(\"\\n-------------------------\\n\")\n",
        "        return inputs\n",
        "\n",
        "\n",
        "def build_rag_chain(llm, retrieval_chain):\n",
        "    prompt_template = \"\"\"You are a helpful and precise AI assistant. Use the information provided in the context below to answer the question accurately and concisely.\n",
        "\n",
        "If the answer cannot be found in the context, reply with \\\"I don't know.\\\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "    rag_chain = (\n",
        "        {\"context\": retrieval_chain, \"question\": itemgetter(\"question\") | RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return rag_chain\n",
        "\n",
        "def compute_metrics(predicted, expected):\n",
        "  exact_match_metric = load(\"exact_match\")\n",
        "  rouge_metric = load(\"rouge\")\n",
        "  meteor_metric = load(\"meteor\")\n",
        "\n",
        "  predicted = list(map(lambda x: str(x) if x is not None else \"\", predicted))\n",
        "  expected = list(map(lambda x: str(x) if x is not None else \"\", expected))\n",
        "\n",
        "  exact_match_metric.add_batch(predictions=predicted, references=expected)\n",
        "  rouge_metric.add_batch(predictions=predicted, references=expected)\n",
        "  meteor_metric.add_batch(predictions=predicted, references=expected)\n",
        "\n",
        "  em = exact_match_metric.compute()[\"exact_match\"]\n",
        "  rouge = rouge_metric.compute()[\"rougeL\"]\n",
        "  meteor = meteor_metric.compute()[\"meteor\"]\n",
        "\n",
        "\n",
        "  return em,rouge,meteor\n",
        "\n",
        "def evaluate(rag_chain, test_data):\n",
        "  all_predicted = []\n",
        "  all_expected = []\n",
        "  results = []\n",
        "  for item in test_data:\n",
        "    question = item['question']\n",
        "    expected = item['answer']\n",
        "    predicted = rag_chain.invoke({\"question\":question})\n",
        "\n",
        "    all_predicted.append(str(predicted) if predicted is not None else \"\")\n",
        "    all_expected.append(str(expected) if expected is not None else \"\")\n",
        "    results.append({\n",
        "        \"question\":question,\n",
        "        \"expected_answer\":expected,\n",
        "        \"predicted_answer\":predicted\n",
        "    })\n",
        "\n",
        "  em, rouge, meteor = compute_metrics(all_predicted, all_expected)\n",
        "  print(f\"\\nAverage Exact Match: {em:.2f}\")\n",
        "  print(f\"\\nAverage Rouge: {rouge:.2f}\")\n",
        "  print(f\"\\nAverage Meteor: {meteor:.2f}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    test_data = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")[\"test\"]\n",
        "    subset = test_data.train_test_split(test_size=0.1, shuffle=True)[\"test\"]\n",
        "\n",
        "    llm, embeddings = initialize_llm_and_embeddings()\n",
        "    docs = load_documents()\n",
        "    splits = split_documents(docs, embeddings)\n",
        "    ensemble_retriever = create_retrievers(splits, embeddings)\n",
        "    reranking_retriever = create_reranking_retriever(ensemble_retriever)\n",
        "    generate_queries = create_query_generator(llm)\n",
        "\n",
        "    retrieval_chain = generate_queries | reranking_retriever.map() | get_unique_union\n",
        "    rag_chain = build_rag_chain(llm, retrieval_chain)\n",
        "\n",
        "\n",
        "    test_results = evaluate(rag_chain= rag_chain, test_data=subset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "y1FTnUB-Qk7g",
        "outputId": "1a25b032-6ef3-4e0d-c893-b0830e067cfd"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def answer_question(question):\n",
        "  answer = rag_chain.invoke({\"question\": question})\n",
        "  return answer\n",
        "\n",
        "llm, embeddings = initialize_llm_and_embeddings()\n",
        "docs = load_documents()\n",
        "splits = split_documents(docs, embeddings)\n",
        "ensemble_retriever = create_retrievers(splits, embeddings)\n",
        "reranking_retriever = create_reranking_retriever(ensemble_retriever)\n",
        "generate_queries = create_query_generator(llm)\n",
        "\n",
        "retrieval_chain = generate_queries | reranking_retriever.map() | get_unique_union\n",
        "rag_chain = build_rag_chain(llm, retrieval_chain)\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Demo\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
